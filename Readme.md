# ğŸ¥ HealthCare Claim Anomaly Detection

## ğŸ“Œ Overview

This project presents a complete data engineering pipeline to detect anomalies in healthcare claims data using a modern data lakehouse architecture. It follows a **Bronze â†’ Silver â†’ Gold** data layer structure using **AWS services**, **Redshift**, and **dashboards for analytics**.

We worked with three real-world healthcare datasets:
- `claims.csv` â€“ ~250,000 records
- `patients.csv` â€“ ~52,000 records
- `providers.json` â€“ ~5,800 records

---

## ğŸ§± Architecture: Bronze - Silver - Gold Layered Pipeline

```
Raw S3 (Bronze) 
     â†“
Preprocessing (Glue + RDS) â†’ S3 (Silver)
     â†“
Redshift Data Warehouse (Gold)
     â†“
Business Analysis & Dashboards
```

---

## ğŸ—ƒï¸ Datasets

| Dataset       | Format | Record Count | Description                      |
|---------------|--------|--------------|----------------------------------|
| claims.csv    | CSV    | ~250,000     | Insurance claims and procedures  |
| patients.csv  | CSV    | ~52,000      | Patient demographics and history |
| providers.json| JSON   | ~5,800       | Provider credentials and details |

---

## ğŸ¥‰ Bronze Layer (Raw Storage)

- All raw files are uploaded to **Amazon S3** in `s3://<bucket-name>/bronze/`
- Structure:
  ```
  s3://<bucket-name>/bronze/
      â”œâ”€â”€ claims.csv
      â”œâ”€â”€ patients.csv
      â””â”€â”€ providers.json
  ```

---

## ğŸ¥ˆ Silver Layer (Preprocessed Data)

### ğŸ”§ Preprocessing Techniques Used

| Dataset       | Method                  | Tool Used       |
|---------------|--------------------------|------------------|
| `claims.csv`  | Visual data mapping, cleaning | AWS Glue Studio |
| `providers.json`| Script-based parsing, flattening | AWS Glue Script Jobs (PySpark) |
| `patients.csv`| SQL-based cleaning (e.g., null handling, type casting) | Amazon RDS (PostgreSQL) |

All cleaned data is stored in:
```
s3://<bucket-name>/silver/
    â”œâ”€â”€ claims_cleaned/
    â”œâ”€â”€ patients_cleaned/
    â””â”€â”€ providers_cleaned/
```

---

## ğŸ¥‡ Gold Layer (Data Warehouse & Analytics)

### ğŸ”„ Loading to Redshift

- Data from **Silver Layer** is ingested into **Amazon Redshift**
- Database: `healthcare_cp`
- Schema: `hcp`

| Table Name       | Description                  |
|------------------|------------------------------|
| `claims_data`    | Cleaned claim details        |
| `patients_data`  | Preprocessed patient info    |
| `providers_data` | Flattened provider metadata  |

---

## ğŸ“Š Business Queries & Dashboards

### ğŸ” Real-World Business Queries

- **Top 10 procedures by volume and revenue**
- **Claims anomalies by provider**
- **Most common diagnoses by region**
- **Average claim amount by location**
- **High-risk patients with repeated claims**
- **Revenue generated per provider**
- **Claim trend analysis over time**
- **Diagnosis code vs procedure code mismatch**
- **Claims without matching provider or patient**
- **Location-wise claim fraud rate**

> All queries were executed in **Amazon Redshift**.

### ğŸ“ˆ Dashboards

- Built interactive dashboards with **Amazon QuickSight / Streamlit / Power BI (choose as applicable)**.
- Charts include:
  - Claim trend over months
  - Top 10 providers by claim count
  - Pie chart: Procedure code distribution
  - Heatmap: Claims by location and diagnosis

---

## ğŸš€ Technologies & Tools Used

| Category         | Tools                         |
|------------------|-------------------------------|
| Cloud Storage    | Amazon S3                     |
| ETL              | AWS Glue Studio, AWS Glue Scripts, Amazon RDS |
| Data Warehouse   | Amazon Redshift               |
| Data Analytics   | SQL, Redshift Queries         |
| Visualization    | Amazon QuickSight / Power BI / Streamlit |
| Programming Lang | Python (for scripts)          |

---

## ğŸ§¼ Anomaly Detection Logic (Overview)

Some common anomaly detection patterns we used:
- **Claim amount deviating from procedure average**
- **Claims with missing or unknown diagnosis codes**
- **Providers with abnormally high claim submissions**
- **Patients submitting claims across multiple regions**
- **Duplicate claims within a short period**

---

## ğŸ“ Project Structure (For GitHub)

```
healthcare-claim-anomaly-detection/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚
â”œâ”€â”€ glue_jobs/
â”‚   â”œâ”€â”€ providers_etl_script.py
â”‚
â”œâ”€â”€ sql_queries/
â”‚   â”œâ”€â”€ analysis_queries.sql
â”‚
â”œâ”€â”€ dashboards/
â”‚   â”œâ”€â”€ dashboard_screenshots/
â”‚   â”œâ”€â”€ dashboard_code/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ requirements.txt
```

---

## âœ… How to Reproduce

1. Upload raw files to S3 `bronze` layer.
2. Run AWS Glue jobs and RDS scripts for preprocessing.
3. Output saved to `silver` S3 layer.
4. Use Redshift COPY commands to ingest from Silver â†’ Redshift tables.
5. Run SQL queries from `sql_queries/analysis_queries.sql`.
6. Visualize with chosen dashboard tool.

---

<!-- ## ğŸ“¬ Contact

For questions or collaboration, reach out to:

**Gowri Swetha**  
Email: your_email@example.com  
LinkedIn: [linkedin.com/in/your-profile](https://linkedin.com/in/your-profile) -->

---
<!-- 
## ğŸ“„ License

This project is licensed under the [MIT License](./LICENSE). -->
